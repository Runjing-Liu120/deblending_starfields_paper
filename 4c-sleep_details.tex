\subsection{Decomposing the sleep objective}
\label{sec:sleep_details}
In this section, we decompose the sleep objective in~\eqref{eq:sleep_obj} for closer study. 
We take $\phi$ as fixed in this section and drop the explicit dependence of $p$ on $\phi$.

First, observe that optimizing the sleep objective does not require computing the intractable term $p(x)$:
\begin{align}
\argmax_\eta\; \mathcal{L}_{sleep}(\eta)
    & = \argmin_{\eta} \; \mathbb{E}_{x \sim p(x)}\Big[ \mathrm{KL}(p(z | x) \| q_\eta(z | x)\Big] \\
  &=\argmin_{\eta} \; \mathbb E_{p(x)}\Big[\mathbb E_{p(z | x)}\Big(\log p(z | x) - \log q_\eta(z | x) \Big)\Big]\\
&=\argmin_{\eta} \; \mathbb E_{p(x)}\Big[\mathbb E_{p(z | x)}\Big( - \log q_\eta(z | x) \Big)\Big]\\
&=\argmin_{\eta} \; \mathbb E_{p(x, z)}\Big[- \log q_\eta(z | x) \Big]\label{eq:sleep_loss_simple}.
\end{align}
Crucially, the integrating distribution, $p(x,z)$, does not depend on the optimization parameter $\eta$.
In the E-step of variational EM, the integrating distribution is $q_\eta$, resulting in the need for reparameterization or other adjustments to compute stochastic gradients. 
Here, unbiased stochastic gradients can be obtained simply as 
\begin{align}
    g = -\nabla_\eta \log q_\eta(z | x) \quad \text{ for } (z, x)\sim p(x, z).
\end{align}

In other words, the sleep phase simulates {\itshape complete} data $(z, x)$ from the generative model and evaluates the loss $-\log q_\eta(z | x)$. 
Here, ``complete data" refers to the image along with its catalog. 
This loss encourages the neural network to map images $x$ to a distribution $q_{\eta}(\cdot | x)$ that places large mass on the catalog $z$.

Furthermore, recall that $q_\eta$ factorizes over image tiles. Having sampled the catalog $z$ 
and the $H\times W$ image $x$ from $p(x,z)$, convert $z$ to its tile parameterization $(\tilde N^{(s,t)}, \tilde \ell^{(s,t)}, \tilde f^{(s,t)})_{s=1,t=1}^{(S,T)}$, as detailed in Section~\ref{sec:factorization}.

For a given image tile, the latent variables $\tilde N^{(s,t)}$, $\tilde \ell^{(s,t)}$, and $\tilde f^{(s,t)}$ also factorize in $q$, so 
\begin{align}
- \log q_\eta(\tilde N^{(s,t)}, 
                \tilde \ell^{(s,t)}, \tilde f^{(s,t)} | x) 
=   - \log q_\eta(\tilde N^{(s,t)} | x) 
        - \log q_\eta(\tilde \ell^{(s,t)} | x) 
        - \log q_\eta(\tilde f^{(s,t)} | x). 
        \label{eq:sleep_loss_decomp}
\end{align}
Each term can be interpreted separately. On tile $(s,t)$, the number of stars $\tilde N^{(s,t)}$ is categorical with parameter $\omega^{(s,t)}$. The loss function for the number of stars becomes
\begin{align}
    - \log q_\eta(\tilde N^{(s,t)} | x) = -\sum_{n = 0}^{\tilde N_{max}} 1\{\tilde N^{(s,t)} = n\} \log \omega^{(s,t)}_n, 
    \label{eq:cross_entropy_loss}
\end{align}
the usual cross-entropy loss for a multi-class classification problem. 

Recall that location coordinates are logit-normal and fluxes are log-normal in the last two terms in~\eqref{eq:sleep_loss_decomp}. 
For a given index $(n,i)$, let $y_{n,i}$ generically denote either the 
logit-location or log-flux for that star, 
and let $\mu_{n,i}$ and $\sigma_{n,i}$ generically denote the mean and standard deviation of its Gaussian variational distribution. Thus,
\begin{align}
    -\log q_\eta(y_{n,i} | x) = 
        \frac{1}{2\sigma^2_{n,i}}(y_{n,i} - \mu_{n,i})^2
         + \log\sigma_{n,i}
         + \frac{1}{2}\log(2\pi).
         \label{eq:gaussian_sleep_loss}
\end{align}
By our discussion in Section~\ref{sec:factorization}, 
only the $N = N^{(s,t)}$-th row of the triangular 
array of latent variables $\tilde \ell^{(s,t)}$ and $\tilde f^{(s,t)}$ needs to be evaluated. Therefore, the losses in the last two terms of~\eqref{eq:sleep_loss_decomp} are of the form 
\begin{align}
    -\log q_\eta(y | x) = -\sum_{i = 1}^{\tilde N^{(s,t)}} \log q_\eta(y_{\tilde N^{(s,t)},i} | x). 
\end{align}

To interpret~\eqref{eq:gaussian_sleep_loss}, observe that
$y_{n,i}$ is the true (simulated) latent variable in the catalog and
$\mu_{n,i}$ is the predicted value for that latent variable from the neural network. 
$\sigma_{n,i}$ is also outputted by the neural network, representing uncertainty -- the second term encourages small uncertainties, but this is 
balanced by the scaling of the error $(y_{n,i} - \mu_{n,i})^2$ in the first term. 

The losses in~\eqref{eq:cross_entropy_loss} and~\eqref{eq:gaussian_sleep_loss} show that the sleep objective results in a supervised learning problem on complete data simulated from our generative model: the objective function for number of stars is the usual cross-entropy loss for classification while the objective function for log-fluxes and logit-locations are $L_2$ losses in the mean parameters. 


% In Section~\ref{sec:estep_sleep_compare}, we will see the benefits of using complete data and optimizing $q_\eta$ using the losses~\eqref{eq:cross_entropy_loss} and \eqref{eq:gaussian_sleep_loss}  rather than the traditional ELBO. 

% \subsubsection{Reverse versus forward KL}
% \label{sec:kl_q_p}
% TODO: something about forward KL overestimates uncertainties, see Figure~\ref{fig:kl_q_p_schematic}. 

% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width = 0.5\textwidth]{figures/kl_q_p_schematic.png}
%     \caption{A toy example where the target distribution $p$ is a bivariate Gaussian on 
%     $z = (z_1, z_2)$ with positively correlated components. 
%     $q$ is a mean-field variational approximation. Left, the optimal $q$ found 
%     optimizing $\KL(q\|p)$; right, the optimal $q$ found optimizing $\KL(p\|q)$. }
%     \jeff{Are the contour lines for $p$ and $q$ the same (e.g., the contour that contains 50\% of the mass)? If so, is it true that these contours touch at exactly two point (rather than 0 or 4)? We should be correct on this detail if we're going to the trouble of showing a plot, even if we're just trying to get intuition.}
%     \label{fig:kl_q_p_schematic}
% \end{figure}

