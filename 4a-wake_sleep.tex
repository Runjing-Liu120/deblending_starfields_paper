% In mean-field variational inference, the ELBO~\eqref{eq:elbo} is often optimized by coordinate ascent in the parameters of the variational distribution. 
% The coordinate ascent updates can be derived in closed form in the special case of exponential family models that are conditionally conjugate~\citep{Blei_2017_vi_review}. 

In the setting of amortized variational inference, 
optimization procedures such as black-box variational inference (BBVI)~\citep{ranganath2013black} and 
automatic-differentiation variational inference (ADVI)~\citep{kucukelbir2016automatic}
have been employed to optimize the ELBO without the need for 
deriving analytic gradients or Hessians. 
% Examples of such ``automatic" procedures include 
%and automatic-differentiation variational inference (ADVI)~\citep{kucukelbir2016automatic}. 
These approaches all sample latent variables from $q_\eta$ and produce an unbiased estimate 
for the gradient of the KL; the optimization is done with stochastic gradient descent. 
ADVI is closely related to the reparameterization trick \citep{kingma2013autoencoding, rezende2014stochastic} proposed to train deep generative models using the KL objective. 

However, the reparameterization trick does not apply when a latent variable -- in our case, the number of stars $N$ -- is discrete. The REINFORCE estimator~\citep{Williams1992reinforce}, which BBVI adapts, produces an unbiased stochastic gradient for both continuous and discrete latent variables.
However, REINFORCE gradients often suffer from high variance in practice, and so the resulting stochastic optimization is slow. We find this to be true in our application as well (Section~\ref{sec:elbo_sleep_compare}). 
% See Appendix~\ref{sec:reparam_details} for details concerning both the reparameterization and REINFORCE estimators. 

The key difficulty in constructing stochastic gradients of the ELBO is that the integrating distribution depends on the optimization parameter $\eta$. 
The {\itshape wake-sleep} algorithm, originally proposed by~\cite{Hinton1995wake_sleep}, replaces the 
ELBO objective with 
\begin{align}
    \mathcal{L}_{sleep}(\eta) := 
    - \Expect_{x \sim p(x)}\Big[\KL(p(z | x) \| q_\eta(z | x))\Big]
    \label{eq:sleep_obj},
\end{align}
known as the {\itshape sleep objective}.  Section~\ref{sec:sleep_details} details a simple gradient estimator for~\eqref{eq:sleep_obj} that does not require reparameterization or REINFORCE. 

There are two key differences between the sleep objective~\eqref{eq:sleep_obj} and the ELBO~\eqref{eq:elbo}. 
First, the KL arguments are transposed. 
Recall that maximizing the ELBO is equivalent to minimizing
$\KL(q\|p)$, and 
therefore, the minimization of $\KL(q\|p)$ does not depend on the intractable data likelihood $p(x)$. 
However, the minimization of $\KL$ with arguments reversed does. 
As will be detailed in Section~\ref{sec:sleep_details}, the outer expectation over the data in~\eqref{eq:sleep_obj} makes this optimization objective tractable without dependence on $p(x)$. 

Second, the expectation over the data also gives different meaning to the sleep objective. The ELBO objective seeks $\eta$ to minimize the $\KL$ between $q_\eta(z | x)$ and $p(z | x)$ {\itshape for fixed, observed data $x$},
in this case the $H\times W$ image.
In contrast, the sleep objective seeks to minimize the $\KL$ {\itshape on average over all possible data $x$}, as weighted by the generative model $p(x)$. 
In other words, the target is no longer an approximate posterior for the observed data, but rather an approximate posterior that is ``good on average" over possible data.

Moreover, ``possible data" is defined under the generative model from Section~\ref{sec:gen_model}. 
Therefore, it is imperative that the generative model $p(x)$ approximates the true underlying data-generating mechanism well. 
Thus, in addition to the sleep phase, the wake-sleep algorithm also incorporates a ``wake-phase" to estimate model parameters.
In our application, these model parameters include the PSF parameters $\pi$ and background parameters $\beta$. Let 
$\phi:=(\pi, \beta)$ be the concatenation of all model parameters, and denote the dependence of the generative model on $\phi$ using subscripts, $p_\phi$. 

To estimate model parameters, one would ideally optimize the marginal log-likelihood $\log p_\phi(x)$.
However, since $\log p_\phi(x)$ is intractable, the wake-phase optimizes for $\phi$ using the ELBO objective~\eqref{eq:elbo}
as a surrogate for the intractable log-likelihood. 
The ELBO is a lower bound of
$\log p_\phi(x)$; 
it is equal to $\log p_\phi(x)$ when $q_\eta(z | x)$ 
= $p_\phi(z | x)$. 

The {\itshape wake-sleep} algorithm thus alternates between the two objectives: 
\begin{align}
    \text{{\bf Sleep phase: }} & 
    \eta_{t} = \argmax_{\eta} -\Expect_{x \sim p_\phi(x)}\Big[\KL(p_{\phi_{t-1}}(z | x) \| q_\eta(z | x))\Big]
    \label{eq:sleep_phase_summary}; 
    \\
    \text{{\bf Wake phase: }} & \phi_{t} = \argmax_{\phi}\; \Expect_{q_{\eta_t}(z | x)}\Big[\log p_{\phi}(x, z) - \log q_{\eta_t}(z | x)\Big],
    \label{eq:wake_phase_summary}
\end{align} 
for iterations $t = 1, ..., T$. 

Stochastic gradients of the expectation in the wake-phase are simple to compute. Because the integrating distribution does not depend on the optimization parameter $\phi$ in the wake phase, unbiased stochastic gradients are simply computed as 
\begin{align}
    \nabla_\phi \log p_\phi(x, z) \quad \text{ for } z\sim q_\eta. 
    \label{eq:mstep_grad}
\end{align}
Section~\ref{sec:sleep_details} shows that a similarly simple gradient estimator exists for the sleep objective.


% While the E-step~\eqref{eq:e_step} is not conducive to simple gradient estimators, we will see in Section~\ref{sec:sleep_details} that the sleep phase objective
% results in a similarly straightforward gradient estimator as in~\eqref{eq:mstep_grad}. 
% In Section~\ref{sec:estep_sleep_compare}, 
% we compare optimizing the E-step with optimizing the sleep phase. In our application of localizing stars,
% the ELBO objective in the E-step suffers from poor local optima where the variational distribution is far from the true posterior in KL divergence; the sleep phase objective appears to better avoid these local optima. 
