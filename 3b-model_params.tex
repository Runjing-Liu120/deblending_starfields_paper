The generative model from Section~\ref{sec:gen_model} requires
the specification of a PSF and a sky background. Let $\phi = (\pi^b, \beta^b)_{b = 1}^B$ be the concatenation of the PSF and background parameters into a real-valued vector. 
% \jeff{It's pretty unusual to use $\phi$ for model parameters, rather than $\theta$. Unless there's a reason, better to use $\theta$. It will be easier on the reader, leaving them with more attention to devote to understanding the substantive ideas in the paper.
% }
% \bryan{with the changes to the gen. model, I ended up using $\theta$ as latent variables :/}
% \jeff{It's pretty common in VI papers to make $z$ all the latent random variables rather than $\theta$.
% That way Greek letters can be used for non-random quantities (model parameters and variational parameters) while Roman letters denote random variables.
% }

Our likelihood, and therefore posterior, now depend on $\phi$. We denote this dependence with a subscript, $p_\phi$. 

We can simultaneously infer a catalog $z$ and 
estimate model parameters $\phi$ by optimizing 
the ELBO in~\eqref{eq:elbo} for both variational parameters $\eta$ and model parameters $\phi$: 
\begin{align}
(\eta^*, \phi^*) = \argmax_{\eta, \phi} \Expect_{q_\eta(z | x)}\Big[\log p_\phi(x, z) - \log q_\eta(z | x)\Big].
\label{eq:em_obj}
\end{align}
Variational EM~\cite{Jordan_intro_vi, neal2000varem, Beal2002varem} optimizes this objective with block coordinate ascent on $\eta$ and $\phi$. Explicitly, 
\begin{align}
    \text{{\bf E-step: }} & 
    \eta_{t} = \argmax_{\eta}\; \Expect_{q_\eta(z | x)}\Big[\log p_{\phi_{t-1}}(x, z) - \log q_\eta(z | x)\Big]; 
    \label{eq:e_step}
    \\
    \text{{\bf M-step: }} & \phi_{t} = \argmax_{\phi}\; \Expect_{q_{\eta_t}(z | x)}\Big[\log p_{\phi}(x, z) - \log q_{\eta_t}(z | x)\Big],
    \label{eq:m_step}
\end{align}
for iterations $t = 1, ..., T$. 

To estimate model parameters, we would ideally optimize the marginal log-likelihood $\log p_\phi(x)$. However, since $\log p_\phi(x)$ is intractable, the M-step optimizes the ELBO as a surrogate for $\log p_\phi(x)$. The ELBO is a lower bound on 
$\log p_\phi(x)$; it is equal to $\log p_\phi(x)$ when $q_\eta(z | x)$ 
= $p_\phi(z | x)$. 

Recall maximizing the ELBO with respect to variational parameters $\eta$ 
is equivalent to minimizing the KL in~\eqref{eq:kl_objective}.
We can thus rewrite the E-step as 
\begin{align}
    \text{{\bf E-step: }} \eta_{t} &= \argmax_{\eta}\; \Expect_{q_\eta(z | x)}\Big[\log p_{\phi}(x, z) - \log q_\eta(z | x)\Big]  \label{eq:e_step_kl1} \\
    &= \argmin_{\eta} \; \mathrm{KL}\Big[\,q_\eta(z | x)\, \| \,p_\phi(z | x )\,\Big].  \label{eq:e_step_kl2}
\end{align} 
In other words, in the E-step, we fix $\phi$ and seek the variational distribution closest to the true posterior under $\phi$. 


% \subsection{Estimation of the catalogue}
% In previous work \cite{Brewer_2013, Portillo_2017, Feder_2019}, the posterior on
% the latent variables $(z, c)$ was approximated using MCMC. In \cite{Portillo_2017, Feder_2019},
% a method was proposed to further reduce the posterior samples to a single point estimate
% which they call a {\itshape condensed catalogue}.
%
% While MCMC allows for the careful quantification of uncertainties, its computational cost
% is prohibitively large for large scale astronomical surveys. One possible alternative
% is to characterize the posterior using the maximum a posteriori estimate. Using the
% generative model from Section~\ref{sec:gen_model}, the joint loglikelihood is
% \begin{align}
%   \log \mathcal{L}(z,& c) \stackrel{c}{=} \overbrace{\sum_{b = 1}^B \sum_{w = 1}^W \sum_{h = 1}^H
%         \Big\{-\frac{1}{2}\log{\lambda_{hw}^b} - \frac{(x_{hw}^b - \lambda_{hw}^b)^2}{2\lambda_{hw}^b}\Big\}}^
%         \text{Gaussian likelihood} + ...\notag\\
%         & ... + \underbrace{N\log(\mu HW) + \log N!}_\text{Poisson prior on $N$} -
%         \underbrace{\sum_{i = 1}^N (\alpha + 1)\log f_{N, i}^b}_\text{Pareto prior on fluxes} +
%         \underbrace{\sum_{b = 1}^B \sum_{i = 1}^N \frac{(c_{N, i}^b - \mu_c)^2}{2\sigma_c^2}}_\text{Gaussian prior on color}
% \end{align}
%
% To maximize this joint-loglikelihood, we must optimize over a discrete random variable $N$,
% so the usual gradient-based optimization methods do not apply. Indeed, it would require
% optimizing the locations, fluxes, and colors for each $N$ independently, and comparing
% the resulting log-likelihoods across $N$.
%
% We propose a method to approximate the maximum a posteriori estimate. Let
% $f:x \mapsto (\hat N, \hat \ell)$ be the function that maps data $x$ to the MAP estimates of
% the $N$ and $\ell$. In our procedure, we first train a neural network $q$ to approximate $f$.
% Thus, obtaining estimates for $N$ and $\ell$ at inference time is a computationally efficient
% forward pass through a neural network. With estimates in hand for $N$ and $\ell$,
% optimizing for $f$ can be done quickly with a few (quasi)-Newton steps; recalling
% our model for the photoelctron counts, equation~\eqref{eq:expected_intensity}, we see that
% are are simply regressing the observed image onto a linear combination of PSFs,
% and the coefficients of this linear combination are the desired fluxes.
