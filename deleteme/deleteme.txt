In traditional variational inference, the divergence between 
the variational posterior $q$ and the true posterior $p$ is 
measured by the ``reverse" KL divergence, defined as the 
$q$-weighted average difference between $\log q$ and $\log p$. In other words, the reverse KL divergence
is defined by an expectation with integrating distribution $q$. 

In simple models, for example when $p$ and $q$ are appropriate classes of exponential families, 
the expectation over $q$ can be written explicitly as a function of the parameters of $q$. Therefore, the minimizer of the KL can be solved either in closed form or by employing deterministic optimization~\cite{Blei_2017_vi_review}. 

In our setting of amortized variational inference, the parameters of $q$ are neural network weights. When analytic expectations are unavailable, as in our case, 
sampling methods have been employed in conjunction with modern auto-differentiation tools to compute stochastic gradients. Optimization is done with stochastic gradient descent. Examples include black-box variational inference (BBVI)~\cite{ranganath2013black} 
and automatic-differentiation variational inference (ADVI)~\cite{kucukelbir2016automatic}. The latter 
is closely related to the reparameterization trick \cite{kingma2013autoencoding, rezende2014stochastic} proposed to train deep generative models using the KL objective. 
These approaches all sample latent variables from $q$ and produce an unbiased estimate 
for the gradient of the KL. 

However, the reparameterization trick does not apply when any latent variables are discrete, in our case, the number of stars. The REINFORCE estimator~\cite{Williams1992reinforce}, which BBVI adapts, produces an unbiased stochastic gradient for both continuous and discrete latent variables. However, the REINFORCE estimator often suffers from high variance in practice, and so the resulting stochastic optimization is slow. 

The key difficulty is that the the objective function is 
an expectation with integrating distribution on $q$, the  distribution to be optimized. To avoid this issue, the {\itshape wake-sleep} algorithm considers the ``forward" KL divergence, defined as the
$p$-weighted average difference between $\log p$ and $\log q$;  in other words, the expectation is taken over $p$. To make the forward KL divergence tractable, a second average is taken over possible data. In other words, the neural network is trained so that {\itshape on average over possible input images $x$}, the 
variational distribution returned by the neural network is close to the true posterior in forward KL divergence. 

Low-variance stochastic gradients are easy to compute using the wake-sleep algorithm, and we show that these low-variance gradients result in faster optimization than using the REINFORCE gradient estimator. 
However, in addition to greater speed, amortized inference may be better at nonconvex optimization: by learning a policy from many examples we learn a policy for avoiding shallow local minima. 



% In simple models, for example when both $p$ and $q$ are related exponential families, the expectation can be written explicitly as a function of the parameters of $q$~\cite{Blei_2017_vi_review}. Therefore, the KL can be minimized either in closed form, or by using deterministic optimization. However, in our case, the parameters of $q$ are neural network weights. In this setting, ~\cite{rezende2014stochastic,kingma2013autoencoding} propose 
% stochastic optimization, where an unbiased gradient of the KL divergence is computed from samples of $q$. 





% One limitation of neural networks is the large amount of data needed for training.
% In astronomy, knowledge of the physical system  often give rise to reasonably realistic simulators for data. For example, simulators were used in \cite{Lanusse_2017_cmudeeplens} trained neural networks to detect Einstein rings, 
% a rare object found in astronomical surveys and used to study dark matter; the neural network in \cite{Hezaveh_2017_nn_lensing_nature} also returned
% morphological parameters for each Einstein ring. 
% In both cases, because the training data was generated from a simulator, the ground truth labels (ring exists or not; the ring morphology) are known. The network is trained in a supervised fashion. Using simulated data is especially useful because the network can be trained on essentially unlimited data -- the bottleneck is the limit on computational resources, not the availability of training data. 

% In our work, we also make use of simulated data to train our neural network. However, we introduce the simulated data in the context of a statistical model. We do so using the wake-sleep algorithm~\cite{Hinton1995wake_sleep}. 
% The neural network outputs are not simply ``labels" for the input image; rather, we are able to interpret the output as specifying an approximate Bayesian posterior in the context of a well-defined statistical model. 

% In summary, we leverage the predictive power and computational efficiency of neural networks to do 
% inference in the framework of a statistical model. 
% We also employ simulated data in our training procedure in a statistically principled way. In Section~\ref{sec:results}, we compare the catalog obtained from our variational posterior with the catalog derived from MCMC; we also compare with traditional cataloging approaches.  

% detect Einstein rings; \cite{Hezaveh_2017_nn_lensing_nature} used
% neural networks to infer morphological characteristics of the Einstein rings; 
% in the context of deblending, 
% \cite{Reiman_2019_gans_deblend} used 
% GANS to separate two overlapping galaxies. 

% Several factors contribute to the success of neural networks. Firstly, neural networks are computationally efficient; multiple images can be easily evaluated in parallel on a GPU.
% Secondly, a well-trained neural network is able to generalize beyond the data on which is was trained. This combined with computational efficiency is extremely beneficial for sky surveys: after training a neural network on a subset of the survey, the remaining images in the survey can be evaluated quickly in batches using only forward-passes through the network. 

% In this paper, we combine the flexibility of neural networks with a formal statistical model. This enables the neural network output to be interpreted in a statistically principled way: the output of our neural network will be a distribution that approximates the Bayesian posterior.

% Secondly, we train the neural network using {\itshape wake-sleep} training. This allows our neural network to be trained using unlimited simulated data. Using simulated data to train neural networks is common practice is astronomy (see for example~\cite{Lanusse_2017_cmudeeplens, huang2019finding}). However, we also make the connection with a formal statistical model 
% and introduce the simulated data in a principled way. TODO: yikes ... this paragraph is terrible and needs work. 

% In Section~\ref{sec:gen_model} we introduce 
% the generative model. Section~\ref{sec:var_inference} details the variational distribution and discuss training of the neural network. Section~\ref{sec:related_work} makes connections with previous cataloging software. In Section~\ref{sec:results}, we compare 
% our variational inference procedure with MCMC as well as more traditional cataloging software pipelines. Section 6 concludes. 

One limitation of neural networks is the large amount of data often needed for training.
In astronomy, knowledge of the physical system sometimes give rise to reasonably realistic simulators for data. 
For example, \cite{Lanusse_2017_cmudeeplens} used simulators to train neural networks to detect Einstein rings, 
a rare object found in astronomical surveys and used to study dark matter; the neural network in \cite{Hezaveh_2017_nn_lensing_nature} also returned
morphological parameters for each Einstein ring. 
In both cases, because the training data was generated from a simulator, the ground truth labels (ring exists or not; the ring morphology) are known. 
The network is trained in a supervised fashion. 
Using realistic simulated data is especially useful because the network can be trained on essentially unlimited data -- the bottleneck is the limit on computational resources, not the availability of training data. 

In our work, we also make use of simulated data for training. However, we introduce the simulated data in the context of a statistical model.
\jeff{I wouldn't even present our work as using simulated data in the first place. It's potentially misleading. We're just using wake-sleep to minimize the forward KL.
Maybe in the discussion section you could talk about the connection between sleep training with training with simulated data, if you think it's helpful for our explaining the good performance.}
In addition, we train the network to provide an approximate Bayesian posterior as discussed in the previous subsection, rather than a simple class label. 




\jeff{
Some of the text below may be useful in the introduction. It explains the challenge in terms statisticians are more likely to connect with.
I haven't published it elsewhere, so feel free to include any parts verbatim if they are useful.

\textit{
Light source separation is challenging for several reasons.
First, it is an unsupervised problem with a sample size of one: there is not labeled data with known ground truth, and there is only one night sky, which is imaged many times.
Second, many stars and galaxies too faint to be unambiguously resolved. For these light sources, providing calibrated uncertainty estimates of our predictions is as important as making accurate predictions.
Third, many stars and galaxies overlap with other light sources---as many as 68\% of galaxies are expected to overlap with others in upcoming LSST survey.
As telescopes improve, even more galaxies will appear to overlap.
Fourth, the scale of the data is immense by any standard. The upcoming LSST survey, scheduled to begin data collection in 2022, is expected to produce 50 petabytes of astronomical images over its lifetime.
}
}



Finally, the likelihood of $x^b_{hw}$, the observed number of photoelectrons at pixel $(h,w)$ and band $b$, is Gaussian
with mean and variance equal to $\lambda^b_{hw}$. 
% This model is reasonable due to the law of rare events and the Gaussian approximation to the Poisson.
% \jeff{If we're actually using a Gaussian rather than a Poisson, it's probably better not to mention the Poisson.
% Just say we model the number as Gaussian with var == mean.}
Thus, the observed pixel intensities are
\begin{align}
  x_{hw}^b | z \overset{ind}{\sim} \mathcal{N}(\lambda^b_{hw}, \lambda^b_{hw}),
  \quad 
  b = 1, ..., B; \;
  h = 1,..., H; \; 
  w = 1, ..., W. 
\end{align}
The scaling of variance with the expected intensity is motivated by the Poissonian nature of photon arrivals. 
% \jeff{Better to use lowercase $x$ rather than $X$ if you're not going to use uppercase for all the random variables}
Explicitly, the log likelihood is
\begin{align}
    \log p(x | z) &= \sum_{b = 1}^{B} \sum_{h = 1}^H \sum_{w = 1}^W 
        \Big\{\frac{1}{2\lambda^b_{hw}}(x_{hw}^b  - \lambda^b_{hw})^2 - 
               \frac{1}{2}\log(2\pi\lambda^b_{hw})\Big\}
    \label{eq:loglik}.
\end{align}

The likelihood and priors described above are similar to those
in previous probabilistic cataloging methods~\cite{Brewer_2013, Portillo_2017, Feder_2019, regier2019_celeste}. 
The only difference between our model and~\cite{Portillo_2017, Feder_2019} is that we use a Poisson prior on $N$, while \cite{Portillo_2017, Feder_2019} use an exponential prior. 

In \cite{Brewer_2013}, a broken power law prior was used for the flux. 
Further hyper-priors were placed on the parameters of the broken power law, and the parameters of the broken power law were treated as latent variables. This would be analogous to placing a hyper-prior on $\alpha$ in \eqref{eq:flux_prior} in our model. 
Alternatively, though not explored in this work, $\alpha$ can be optimized as a model parameter along with the PSF and sky background. 
The same principle applies to the Poisson prior parameter, $\mu$. 



% In catalog M2, we use the same prior parameters as~\cite{Portillo_2017, Feder_2019}: the slope of the power law on fluxes is set at  $\alpha = 2$; the minimum flux $f_{min}$ to corresponds to the SDSS 
% lower detection limit ($\approx$ 22 magnitude); and the color prior has a standard Gaussian prior, which only disfavors extremely atypical colors, $|c_{i,b}| > 3$. 

% One difference is that \cite{Portillo_2017, Feder_2019} use an exponential prior on $N$. We use a Poisson prior and set $\mu = 0.15$, slightly denser than the $0.1$ stars per pixel 
% brighter than 22mag found in the Hubble catalog of M2. Appendix TODO shows that our model is insensitive to the choice of Poisson mean. 



\jeff{I suggest writing the parameters of these distributions as functions of the padded tile. For example, I'd write
\[
    \tilde N_{rt} \sim \text{Categorical}\left(h_\eta(\tilde{x}_{rt})\right).
\]
Here $h_\eta$ is a neural network. ($\eta$ are variational parameters).
I didn't drop the $rt$ subscript above because I wanted to make the point that the same $h_\eta$ is applied to all tiles.
}
\bryan{
I like this idea. I can introduce {\itshape padded} tiles,  
\[
\hat x^{(s,t)} = 
    \{x_{hw} : R(s+1) + P \geq h > Rs - P \text{ and } 
    R(t+1) + P\geq w > Rt - P\}.
\]
That is, we took the tiles from~\eqref{eq:tiles}
and padded each tile with $P$ pixels. Note
that tiles are disjoint, while padded tiles are not. 

I want to say that a neural network 
$h_\eta(\hat x^{(r,s)})$ that maps 
to a common concatenated vector of 
parameters describing the distributions. 

We index into this giant vector to get individual distributional parameters. 
I'm not sure how quite to denote this though ... for example to get the mean 
parameter for the $(N,i)$-th location, we'd have $\mu_{\ell_{N, i}}(h_\eta(\hat x^{(r,t)}))$, which seems like too many indices.
This is the problem with this section in general tbh ... more work will be needed on my part to figure out how to describe this. 
}
\jeff{I see your point, the notation is tricky. The main issue with $\mu_{\ell_{N, i}}(h_\eta(\hat x^{(r,t)}))$ is the double subscript. Also, it's not great have $\ell$ as a subscript of $\mu$ because $\ell$ is a random variable.
}
\jeff{
Maybe  $\left[g_\eta(\tilde x_{rt})\right]_{N,i}$ could denote the location mean and variance for the $(N,i)$-th location?
And $\left[h_\eta(\tilde x_{rt})\right]_{N,i}$ could denote the flux mean and variation?
}

% Traditionally in variational inference, the posterior approximation 
% $q_\eta$ depends on the data, the observed image $x$, implicitly, 
% in that $\eta^*$ is chosen according to~\eqref{eq:kl_objective}.
% In this case, $q_\eta(z | x)$ is usually written $q_\eta(z)$, suppressing the dependence on $x$

% Astronomical surveys consist of many images $x_1, ..., x_n$. 
% Assume $x_1, ..., x_n$ are drawn i.i.d.~from the same model 
% (e.g.~the images were taken from a field with homogeneous source density). 
% The optimization problem~\eqref{eq:kl_objective} must be solved for each $x_i$ to find an approximation to the posterior $p(z_i | x_i)$. 

% Conversely, in {\itshape amortized} variational
% inference~\cite{kingma2013autoencoding, rezende2014stochastic}, $q_\eta$ explicitly depends on the observed image $x$. A flexible, parameterized function, typically a neural network, maps the input image $x$ to
% the parameters of a distribution on $z$. 
% The variational parameters $\eta$ in~\eqref{eq:kl_objective} 
% are the neural network weights. 
% After the neural network is fitted using~\eqref{eq:kl_objective} with $x = x_1$,
% each approximate posterior $q_\eta(z_i | x_i)$, $i = 2, ..., n$ 
% can be evaluated with a single forward pass through the neural network. 
% No additional run of an iterative optimization routine is needed. 



Section~\ref{sec:sleep_details} shows that \eqref{eq:sleep_obj} is 
equivalent to minimizing 
\begin{align}
\mathcal{L}_{sleep}(\eta) = \mathbb E_{p(x, z)}\Big[- \log q_\eta(z | x) \Big] + C
\end{align}
where $C$ is a constant independent of $\eta$. 
The integrating distribution does not depend on $\eta$, and Section~\ref{sec:sleep_details} details a simple gradient estimator that does not require reparameterization or REINFORCE. 
