\subsection{Runtime} 
\label{sec:runtime}
We ran the wake-sleep algorithm on $100 \times 100$ images. 
% \jeff{An encoder network can be trained, but not a variational approximation.
% A variational approximation can be ``selected''. Or the variational parameters can be fit/trained.
% }
However, our variational posterior is defined on images of arbitrary size
% \jeff{Our variational approximation to the posterior is \textit{defined} for images of arbitrary size. It's not ``evaluated''.}
by tiling the image into $s \times s$ tiles. On the crowded starfield described below, we used $s = 2$. 

We ran the initial sleep phase~\eqref{eq:sleep_phase_summary}
% \jeff{What does it mean to ``train'' a sleep phase?}
with the PSF and background set to SDSS estimates. We trained for 200 epochs with 200 images per epoch with  
the Adam optimization routine~\cite{kingma2014adam}. 
% \jeff{The defaults are specific to PyTorch, not Adam. If you are going to talk about the hyperparameter settings, at least say what ``default'' means.}
% \jeff{That said, it's probably better not to mention how you set the parameters---too much detail. The reader will trust that you did something reasonable, and they can check out your open source code if they want this level of detail.}
On a single Nvidia GeForce RTX 2080 Ti GPU 
the initial sleep phase took approximately 10 minutes. 

We ran two further wake-sleep cycles. Each cycle took approximately a minute. The subsequent sleep phases were shorter (10 epochs with 200 images each), and the wake phase employed SGD, with the gradient estimator in \eqref{eq:mstep_grad}. 

After training, evaluating the approximate posterior on a $100 \times 100$ 
image took $\approx 0.2$ seconds. By comparison, the reported runtime of PCAT, which uses MCMC, is 30 minutes on the same $100 \times 100$ subimage of M2~\cite{Feder_2019}. 

The speed of our procedure at inference time (which excludes training time) enables our method to scale up to large astronomical surveys. A single SDSS image is $1489 \times 2048$ pixels. Projecting the reported runtime of PCAT to the full image, 
we would expect a runtime of 
$\approx 30\text{min} \times 14 \times 20 = 8400$min, or almost six days.
\jeff{What would be the runtime of PCAT for all of SDSS? You can get the relevant information about SDSS here: \url{https://www.sdss.org/dr13/scope/}.
It might be information to do this calculation for LSST too. (See lsst.org)
}

In contrast, if we assume the PSF and background are homogeneous 
across the full SDSS image (which PCAT also assumes), we can 
train our variational approximation using wake-sleep 
on a small $100 \times 100$ subimage
(while getting estimates of the PSF and background along the way),
a one time computational cost of twelve minutes. 
Producing a catalog on the full $1489 \times 2048$ image will require 
$\approx 0.2\text{sec} \times 14 \times 20 = 56$ seconds. In practice, 
this will be even faster as we would batch the image tiles to run in parallel on a GPU. 

Moreover, an SDSS image is indexed by a run, camera column (camcol), and field.
A run is one continuous scan of the telescope, usually corresponding to one night of data collection. 
A run is broken down into fields; some runs have over 800 fields. 
Each field contains six camera columns. 
Thus, for a large-scale sky survey like SDSS which 
contains over 8000 runs, MCMC will be infeasible. 


\subsection{Inference on the M2 globular cluster}
We applied our method to the SDSS image of the M2 globular cluster found in run 2583, camcol 2, and field 136.
M2 was also imaged in the ACS Globular Cluster Survey~\cite{Sarajedini_2007}
using the Hubble Space telescope (HST),
which has $\approx20$ times the angular resolution and $\approx30$ times the exposure of the Sloan telescope. The reported catalog from this Hubble survey was used to 
validate our results.

We focused on a specific $100 \times 100$ subimage of M2 that \cite{Portillo_2017, Feder_2019} cataloged using PCAT.
This subimage is located $\approx2$ arcseconds away from the heavily saturated core of the cluster;
even in this subimage, the HST catalog contained over 1000 stars brighter than the 22nd magnitude in its F606W band.

We compared our method with PCAT and DAOPHOT. The DAOPHOT catalog of M2 was reported in 
\cite{An_2008_m2}. While DAOPHOT returned photometry for all five SDSS bands, 
we chose to model only the $r$ and $i$ bands for our method and PCAT. We found that for our method and PCAT, the inclusion of more bands did not significantly improve
the quality of our detections on metrics defined below. 
\jeff{It didn't? That seems like a problem. Did it improve our true negative rate at least?}
\bryan{For Portillos, they said it was because the 
alignment problem became more pronounced as bands were added, and they didn't even report results for three bands. I tried three bands, and our results were similar to the two band results. }
\jeff{You might examine a pair of blended stars we don't get right with just one band, where the colors of the stars are very different. Wouldn't using multiple bands rather than just one let us correctly deblend this pair?}
% \jeff{What is positive predictive value? Is that what's described below? Better to define the term before using it.}

Both DAOPHOT and the Hubble survey reported a ``traditional" catalog, that is, a list of stellar locations and their respective fluxes.
\jeff{Do these catalogs contain any uncertainty estimates?}
In the context of probabilistic cataloging (our method and PCAT), the posterior 
defines a distribution over catalogs -- each draw from the posterior returns a catalog 
(of potentially varying length) of stellar locations and fluxes. 

Given an estimated catalog, two metrics we examined were the true positive rate (TPR) and the positive predicted value (PPV). Using the HST catalog as ground truth, the TPR is the proportion of stars in the HST catalog that had a match in the estimated catalog;
the PPV is the proportion of stars in the estimated catalog that had a match in the HST catalog. Like \cite{Portillo_2017, Feder_2019}, we defined a match to be when the estimated location and the HST location were within 0.75 pixels
(TODO: results actually right now are for 0.5 pixels),
and the estimated $r$-band flux and the HST F606W band flux were within half a
magnitude. The slack in the magnitude allows for some discrepancy between the 
SDSS $r$-band and the the Hubble F606W band. The Hubble F606W band absorption range is broader than the SDSS $r$-band, though they centered at roughly the same wavelength. 

We filtered the Hubble catalog to stars brighter than 22.5~mag in the F606W band, corresponding to the detection limit in SDSS images  
(TODO: we should make this precise. Portillos says something like this, too). In our method and PCAT, this amounted to setting $f_{min}$ in the flux prior (equation~\ref{eq:flux_prior})
to the 22.5th magnitude; all dimmer stars were absorbed into the sky intensity. 

Table~\ref{tab:summary_stats} reports the PPV and TPR for the considered cataloging methods.
% \jeff{Summary statistics of what? Could you say something like ``Table~\ref{tab:summary_stats} reports the performance of these methods'' instead? That would be more descriptive.}
For our method, the TPR and PPV were computed at the maximum a posteriori (MAP) catalog under
the variational distribution. 
\jeff{A lot of sentences that have ``we'' as the subject can be rewritten in way that doesn't mention us (without switching to passive voice, which often doesn't sound good). It makes the writing easier to understand because typically ``we'' are not really central to the idea we're trying to convey. 
So you might say thing like ``Our method performed better than PCAT in terms of TPR and PPV'' rather than all four of the follow sentences: ``We compute the the TPR and PPV for our method. We compared it to PCAT. We found that our method performed better than PCAT. Note that we used the MAP estimate to compute TPR.''
}
We compared the variational distribution obtained using only the sleep-phase (optimizing~\eqref{eq:sleep_obj} at the SDSS estimates of the model parameters)
and the variational distribution obtained after two further cycles of wake-sleep. The additional two cycles of wake-sleep improved the PPV 
over sleep-only by 17\%; the TPR decreases slightly by 2\%. 
The catalog did not improve after the second iteration of wake-sleep. 

For PCAT, we computed the summary statistics for each catalog sampled from their posterior and display the average over 300 posterior samples. 
The wake-sleep PPV was much better than PCAT. 
Further, even though PCAT predicted roughly 50\% more stars than 
wake-sleep, its TPR was not better.
% \jeff{Can we drop the word ``significantly'' here?}

% , and ran 
% a first sleep phase (optimizing equation~\ref{eq:sleep_obj}). We compare the 
% catalog obtained under the MAP estimate under the variational distribution trained using only this sleep phase against the 
% variational distribution obtained after two further cycles of wake-sleep. 
% We define the ``wake-sleep" catalog 

% Our  MAP  estimate  under  the  variational  distribution  trained  using  wake-sleep
% (which we will call our “wake-sleep catalog”) had similar TPR to the 


% (which we will call our ``wake-sleep catalog")


Conversely, the DAOPHOT catalog underestimated the number of stars in this submimage, and it had the lowest TPR of all the methods.
Further, while wake-sleep predicted nearly three times as many stars as DAOPHOT, wake-sleep only suffered a decrease in PPV of 4\%. 

We combined TPR and PPV into one statistic using the F1 score,
defined as the geometric average between TPR and PPV, and found that both sleep-only and wake-sleep outperform PCAT and DAOPHOT.

\input{tables/summary_stats_m2.txt}

Figure~\ref{fig:summary_stats} shows the TPR and PPV binned by magnitude.
Wake-sleep and PCAT showed similar TPR across all magnitudes; both were
uniformly better than DAOPHOT. Wake-sleep had a PPV higher than PCAT for all magnitudes. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/summary_statistics_m2.png}
    \caption{True positive rate and positive predicted value of various cataloging
    procedures on M2, plotted against magnitude percentile.
    Smaller magnitudes correspond to brighter stars. }
    \label{fig:summary_stats}
\end{figure}

Figure~\ref{fig:example_subimages} shows four $10\times10$ subimages, and overlays our wake-sleep MAP catalog on the Hubble catalog. We also display the DAOPHOT catalog and one catalog sample from PCAT.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/example_subimages.png}
    \caption{Estimated catalogs on four 10$\times$10 subimages from
    M2. Blue dots are Hubble stars brighter than the 22nd magnitude.
    Wake-sleep, PCAT, and DAOPHOT estimated stars are in
    red, cyan, and orange x's, respectively. }
    \label{fig:example_subimages}
\end{figure}


Figure~\ref{fig:example_subimages_sampled} shows sampled catalogs from both our wake-sleep approximate posterior as well as from PCAT.
Our variational approximate posterior appeared to show larger uncertainties
than PCAT. This is in agreement from the discussion of section~\ref{sec:kl_q_p}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/example_subimages_samples.png}
    \caption{Four 10$\times$10 subimages from
    M2. Blue dots are Hubble stars brighter than the 22nd magnitude.
    We print display the posterior samples from our wake-sleep variational
    posterior (left) and from the MCMC chain of PCAT (right). }
    \label{fig:example_subimages_sampled}
\end{figure}

Figure~\ref{fig:z-score_calibration} examines the calibration of our uncertainties. We evaluated our wake-sleep approximate posterior 
conditional on the true number of stars in the Hubble catalog. 
Thus, each star in our estimated catalog was matched with exactly one Hubble star (we found the permutation of the Hubble stars that had the largest log-likelihood under our variational distribution $q_\eta$). For each star, we computed the z-score $(y - \hat y) / \hat \sigma$, where $y$ is the true log-flux or 
logit-location; $\hat y$ is the mean of the Gaussian variational distribution, and $\hat\sigma$ the standard deviation.
If the uncertainties were perfectly calibrated, the distribution of the z-scores would be a standard Gaussian. 
We see from Figure~\ref{fig:z-score_calibration} that 
the tails of our empirical z-score distribution are slightly larger than standard Gaussian, suggesting that our error is somewhat under-estimated. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/z-score_calibration.png}
    \caption{The calibration of uncertainties in our variational posterior. Conditional on the true number of stars, we computed the z-score of the true logit-location or log-flux evaluated at our variational posterior. 
    The empirical mean and variance for the logit-location z-scores are 0.27 and 1.91, respectively; The empirical mean and variance for the log-flux z-scores are 0.17 and 1.53, respectively. 
}
    \label{fig:z-score_calibration}
\end{figure}

Figure~\ref{fig:luminosity_fun_m2} shows the distribution of estimated $r$-band fluxes. We compared them 
with the distribution of F606W-band fluxes in the HST catalog. While the Hubble F606W-band has a broader absorption range than the SDSS $r$-band, they are centered at roughly the same wavelength. For PCAT, we displayed the flux distribution from single catalog sample; for our wake-sleep variational 
posterior, we used our MAP catalog. DAOPHOT returned fluxes only up to $\approx21$ mag.
Conversely, PCAT greatly over-estimated the number of dim stars with magnitude less than 21 mag. Our flux distribution came closest to the distribution of the HST catalog. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/luminosity_fun.png}
    \caption{Source magnitude histograms on M2. }
    \label{fig:luminosity_fun_m2}
\end{figure}

Finally, Figure~\ref{fig:cmd_m2} displays the estimated color-magnitude diagrams. While the Hubble F606W-band corresponds roughly to the SDSS $r$-band, there is no such correspondence for the SDSS $i$-band. Thus, using the Hubble locations, we estimated the ``true" $i$-band fluxes using maximum likelihood; that is, we maximized \eqref{eq:loglik}
for $f$, with $N$ and $\ell$ set to the true number of stars and locations. We used this estimated $i$-band flux and the Hubble F606W-band flux to define the ``true" color. We again compared with the catalog returned by DAOPHOT, PCAT, and wake-sleep. We see that DAOPHOT did not capture the full spectrum of colors; of all three methods, PCAT seemed to best capture the color spectrum. All three methods however, were able to capture the arm thing (TODO there is a name for this ... main sequence turnoff?) that branches off at low magnitudes. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/cmd.png}
    \caption{Color magnitude diagrams on M2. }
    \label{fig:cmd_m2}
\end{figure}


\subsection{Estimation of model parameters}
\label{sec:results_model_params}
In this section, we examine our estimated parameters 
for the sky intensity and the PSF. As shown in the previous section, wake-sleep improved the catalog over sleep-only. In Table~\ref{tab:chi-square-stats1}, we
print the negative log-likelihood $- \log \Prob_\phi(X | N_{H}, \ell_{H}, f_{H})$ for various model parameters $\phi$, where $N_{H}, \ell_{H}, f_{H}$ are the ground truth
values from the Hubble catalog (though the $i$-band flux needs to be estimated, as above). 

We compared the log-likelihood under the default SDSS estimates with the estimates found using wake-sleep. Wake-sleep estimates improved the log-likelihood. We also compared against the ``Hubble estimate" of the background and PSF, obtained by minimizing 
$- \log \Prob_\phi(X | N_{H}, \ell_{H}, f_{H})$ for $\phi$ directly. 

This table suggests that the largest source of model misfit is the background. A significant decrease in negative log-likelihood occurred by switching from the SDSS background to the Hubble-estimated background. 
But even using the Hubble-estimated background, switching from the SDSS PSF to our wake-sleep PSF still improved the log-likelihood. 

\input{tables/chi_sq_stats.txt}
% \caption{
% Negative log-likelihood for SDSS, wake-sleep, and Hubble estimated model parameters. In the right column, we fix the background to the Hubble estimate, and examine negative log-likelihood as the PSF varies.}

As evidenced in Figure~\ref{fig:psf_profiles}, our wake-sleep PSF did not change the SDSS PSF significantly. The greatest change was in the $r$-band PSF, where the SDSS PSF was most different from the Hubble-estimated PSF. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/psf_profiles.png}
    \caption{Estimated versus true PSF profiles on M2. The Hubble PSF was
    obtained by optimizing the likelihood conditioned on locations and fluxes
    from the Hubble catalog. }
    \label{fig:psf_profiles}
\end{figure}


% \multicolumn{1}{p{5cm}}{\raggedleft Neg. loglik \\ (with Hubble back.)}
% \caption{
% Chi-squared statistics for SDSS, wake-sleep, and Hubble estimated model parameters.
% The chi-squared statistic is defined as
% $\sum_{bij}\frac{([\text{obs.image}]_{bij} - [\text{recon.image}]_{bij})^2}{[\text{recon.image}]_{bij}}$.
% In the middle column, ``model parameters" refer to both background and PSF.
% In the right column, we fix the background to the Hubble estimate, and examine
% chi-squared statistics as the PSF varies.}

\subsection{Sparse field test}
As discussed in Section~\ref{sec:runtime}, our approach has the potential 
to scale up to large astronomical surveys. We chose the image of M2 to test our method because ``ground truth" could be obtained from Hubble images to validate our results. Moreover, 
the PSF model is fairly realistic on M2: all sources are 
stars, and there are no galaxies in this region. 
However, most regions of the sky are much less 
crowded than M2.


We chose SDSS run 94, camcol 1, and field  12 to examine our method on a sparse field more typical of SDSS 
images. After ten minutes of sleep training, we produced a catalog on the full $1489\times 2048$ image in $\approx2$ seconds (versus approximately six days of runtime for MCMC). 

This image is contained in Stripe 82, a region of the sky repeatedly imaged by SDSS. Averaging images from different runs boosts the signal to noise ratio and this {\itshape co-added} image can be analyzed to obtain a ground truth. 

However, this region of the sky also contains galaxies, which are 
not well-modelled by a PSF. A future paper will extend our generative model to include galaxies. For our setup, we use our approximate posterior 
trained using sleep-only (otherwise the model will try to use the PSF to explain both stars and galaxies in the wake-phase). 

Since this region of the sky is more sparse, we tile the image into $50\times 50$ tiles; $N_{max}$ on the tiles is three. 

Figure~\ref{fig:sparse_field} shows a $500\times 500$ subimage with 
our MAP catalog overlayed with the ground truth. We also report the 
TPR, comparing the stars in the co-added catalog with our estimated stars. (We have extraneous defections that are due to galaxies and thus 
do not report the PPV). Of the stars in the co-added catalog with brightness at least $22.5$mag, we achieved a TPR of 90\%. 


\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/sparse_field_detections.png}
        \label{fig:sparse_field_detect}
    \end{subfigure}
    \begin{subfigure}{0.54\textwidth}
        \includegraphics[width=\textwidth]{figures/sparse_field_tpr.png}
        \label{fig:sparse_field_tpr}
    \end{subfigure}
    \caption{(Left) a $500\times 500$ sparse field, with true stars in 
    blue, and true galaxies in green. Estimated stars are shown in red x's. 
    (Right) the true positive rate as a function of true magnitude. }
    \label{fig:sparse_field}
\end{figure}
