\subsection{Runtime} 
\label{sec:runtime}
In the sleep phase, we trained the variational approximation on 
$100 \times 100$ images. However, our variational posterior can be evaluated on images of arbitrary size, by tiling the image into $s \times s$ tiles. On the crowded starfield described below, we used $s = 2$. 

We trained the initial sleep phase with the PSF and background set to SDSS estimates. We trained for 200 epochs with 200 images per epoch. We used 
the Adam optimization routine~\cite{kingma2014adam} set at its default parameters. On a single Nvidia GeForce RTX 2080 Ti GPU 
this initial sleep phase took approximately 10 minutes. 

We ran two further wake-sleep cycles with each cycle taking approximately a minute. The subsequent sleep phases were shorter (10 epochs with 200 images each), and the wake phase employed SGD, with the gradient estimator in \eqref{eq:mstep_grad}. 

After training, evaluating the approximate posterior on a $100 \times 100$ 
image took $\approx 0.2$ seconds. By comparison, the reported runtime of PCAT, which uses MCMC, is 30 minutes on a $100 \times 100$ subimage of M2~\cite{Feder_2019}. 

We emphasize that our speed at inference time is what enables our method to 
scale up to large astronomical surveys. A single SDSS image is $1489 \times 2048$ pixels. Projecting the reported runtime of PCAT to the full image, 
we would expect a runtime of 
$\approx 30\text{min} \times 14 \times 20 = 8400$min, or almost six days. 

In contrast, if we assume the PSF and background are homogeneous 
across the full SDSS image (which PCAT also assumes), we can 
train our variational approximation using wake-sleep 
on a small $100 \times 100$ subimage
(while getting estimates of the PSF and background along the way),
a one time computational cost of twelve minutes. 
Producing a catalog on the full $1489 \times 2048$ image will require 
$\approx 0.2\text{sec} \times 14 \times 20 = 56$ seconds. In practice, 
this will be even faster as we would batch the image tiles to run in parallel on a GPU. 

Moreover, an SDSS image is indexed by a run, camera column (camcol), and field.
A run is one continuous scan of the telescope, usually corresponding to one night of data collection. 
A run is broken down into fields; some runs have over 800 fields. 
Each field contains six camera columns. 
Thus, for a large-scale sky survey like SDSS which 
contains over 8000 runs, MCMC will be unfeasible. 


\subsection{Inference on M2}
To test our performance, we applied our method to the SDSS image of M2 found in run 2583, camcol 2, and field 136.
M2 was also imaged in the ACS Globular Cluster Survey~\cite{Sarajedini_2007}
using the Hubble Space telescope (HST),
which has $\approx20$ times the angular resolution and $\approx30$ times the exposure of the Sloan telescope. The reported catalog from this Hubble survey was used to 
validate our results.

We focused on a specific $100 \times 100$ subimage of M2 that \cite{Portillo_2017, Feder_2019} cataloged using PCAT.
This subimage is located $\approx2$ arcseconds away from the heavily saturated core of the cluster;
even in this subimage, the HST catalog contained over 1000 stars brighter than the 22nd magnitude in its F606W band.

We compared our method with PCAT and DAOPHOT. The DAOPHOT catalog of M2 was reported in 
\cite{An_2008_m2}. While DAOPHOT returned photometry for all five SDSS bands, 
we chose to model only the $r$ and $i$ bands for our method and PCAT. We found 
that for our method and PCAT, the inclusion of more bands did not improve
the true positive rate and positive predictive value (described below) of our detections. 

Both DAOPHOT and the Hubble survey reported a ``traditional" catalog, that is, a list of stellar locations and their respective fluxes. In the context of probabilistic cataloging (our method and PCAT), the posterior 
defines a distribution over catalogs -- each draw from the posterior returns a catalog 
(of potentially varying length) of stellar locations and fluxes. 

Given an estimated catalog, two metrics we examined were the true positive rate (TPR) and the positive predicted value (PPV). Using the HST catalog as ground truth, the TPR is the proportion of stars in the HST catalog that had a match in the estimated catalog;
the PPV is the proportion of stars in the estimated catalog that had a match in the HST catalog. Like \cite{Portillo_2017, Feder_2019}, we defined a match to be when the estimated location and the HST location were within 0.75 pixels
(TODO: results actually right now are for 0.5 pixels),
and the estimated $r$-band flux and the HST F606W band flux were within half a
magnitude. The slack in the magnitude allows for some discrepancy between the 
SDSS $r$-band and the the Hubble F606W band. The Hubble F606W band absorption range is broader than the SDSS $r$-band, though they centered at roughly the same wavelength. 

We filtered the Hubble catalog to stars brighter than 22.5~mag in the F606W band, corresponding to the detection limit in SDSS images  
(TODO: we should make this precise. Portillos says something like this, too). In our method and PCAT, this amounted to setting $f_{min}$ in the flux prior (equation~\ref{eq:flux_prior})
to the 22.5th magnitude; all dimmer stars were absorbed into the sky intensity. 

Table~\ref{tab:summary_stats} prints the summary statistics. For our method, we  computed the TPR and PPV at the maximum a posteriori (MAP) catalog under
the variational distribution. 
We compared the variational distribution obtained using only the sleep-phase (optimizing~\eqref{eq:sleep_obj} at the SDSS estimates of the model parameters)
and the variational distribution obtained after two further cycles of wake-sleep. The additional two cycles of wake-sleep improved the PPV 
over sleep-only by 17\%; the TPR decreases slightly by 2\%. 
We found that the quality of the catalog did not seem to further improve after the second iteration of wake-sleep. 

For PCAT, we computed the summary statistics for each catalog sampled from their posterior and display the average over 300 posterior samples. 
Our wake-sleep PPV was much better than PCAT. 
Moreover, PCAT predicted roughly 50\% more stars than 
wake-sleep, without significant gains in the TPR. 

% , and ran 
% a first sleep phase (optimizing equation~\ref{eq:sleep_obj}). We compare the 
% catalog obtained under the MAP estimate under the variational distribution trained using only this sleep phase against the 
% variational distribution obtained after two further cycles of wake-sleep. 
% We define the ``wake-sleep" catalog 

% Our  MAP  estimate  under  the  variational  distribution  trained  using  wake-sleep
% (which we will call our “wake-sleep catalog”) had similar TPR to the 


% (which we will call our ``wake-sleep catalog")


Conversely, we found that the DAOPHOT catalog underestimates the number of stars in this submimage, and it has the lowest TPR of all the methods. Moreover, while wake-sleep predicted nearly three times as many stars as DAOPHOT, 
wake-sleep only suffered a decrease in PPV of 4\%. 

We combined TPR and PPV into one statistic using the F1 score,
defined as the geometric average between TPR and PPV, and found that both 
sleep-only and wake-sleep outperform PCAT and DAOPHOT.

\input{tables/summary_stats_m2.txt}

In figure~\ref{fig:summary_stats}, we plot the TPR and PPV binned by magnitude.
Wake-sleep and PCAT showed similar TPR across all magnitudes, and both were
uniformly better than DAOPHOT. Wake-sleep had a PPV that is higher than PCAT for all magnitudes. Of all methods, DAOPHOT had the best PPV at smaller magnitudes.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/summary_statistics_m2.png}
    \caption{True positive rate and positive predicted value of various cataloging
    procedures on M2, plotted against magnitude percentile.
    Smaller magnitudes correspond to brighter stars. }
    \label{fig:summary_stats}
\end{figure}

In figure~\ref{fig:example_subimages}, we display four $10\times10$ subimages, and plot
our wake-sleep catalog against the Hubble catalog. We also display the DAOPHOT catalog and one catalog sample from PCAT.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/example_subimages.png}
    \caption{Estimated catalogs on four 10$\times$10 subimages from
    M2. Blue dots are Hubble stars brighter than the 22nd magnitude.
    Starnet, PCAT, and DAOPHOT estimated stars are in
    red, cyan, and orange x's, respectively. }
    \label{fig:example_subimages}
\end{figure}


In figure~\ref{fig:example_subimages_sampled}, we display the sampled catalogs from both our 
wake-sleep trained approximate posterior as well as from PCAT. Assuming the MCMC
of PCAT converged to the true posterior, we see that our uncertainty estimates
were generally larger. This is expected from the discussion of section~\ref{sec:kl_q_p}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/example_subimages_samples.png}
    \caption{Four 10$\times$10 subimages from
    M2. Blue dots are Hubble stars brighter than the 22nd magnitude.
    We print display the posterior samples from our variational
    posterior (left) and from the MCMC chain of PCAT (right). }
    \label{fig:example_subimages_sampled}
\end{figure}

In figure~\ref{fig:z-score_calibration} we examine the calibration of our uncertainties. We evaluated our approximate posterior obtained from wake-sleep training, conditioning on the true number of stars in the Hubble catalog. Thus, each star in our estimated 
catalog was matched with exactly one Hubble star (we found the permutation of the Hubble stars that had the largest log-likelihood under our variational distribution $q_\eta$). For each star, we computed the z-score $(y - \hat y) / \hat \sigma$, where $y$ is the true log-flux or 
logit-location; $\hat y$ is the mean of the Gaussian variational distribution, and $\hat\sigma$  its standard deviation. If the uncertainties were perfectly calibrated, the distribution of the 
z-scores would be a standard Gaussian. We found the empirical variance of the z-scores
to be approximately one (TODO: report values). There was a slight upward downward bias in our estimated fluxes, suggesting some degree of model mis-specification. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/z-score_calibration.png}
    \caption{The calibration of uncertainties in our variational posterior. Conditional on the true number of stars, we computed the z-score of the true logit-location or log-flux evaluated at our
    variational posterior. }
    \label{fig:z-score_calibration}
\end{figure}

We plot the distribution of estimated r-band fluxes in figure~\ref{fig:luminosity_fun_m2}. We compared 
against the distribution of F606W-band fluxes in the HST catalog. While the Hubble F606W-band has a broader absorption range than the SDSS r-band, they are centered at roughly the same wavelength. For PCAT, we display the flux distribution from single catalog sample; for our variational 
posterior, we use our MAP catalog. We see that DAOPHOT returned fluxes only up to $\approx21$ mag; 
conversely, PCAT greatly over-estimated the number of dim stars with magnitude less than 21 mag. Our flux distribution came closest to the distribution of the HST catalog. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/luminosity_fun.png}
    \caption{Source magnitude histograms on M2. }
    \label{fig:luminosity_fun_m2}
\end{figure}

Finally, we print the estimated color-magnitude diagrams in figure~\ref{fig:cmd_m2}. While the Hubble F606W-band corresponds roughly to the SDSS $r$-band, there is no such correspondence for the SDSS $i$-band. Thus, using the Hubble locations, we estimated the ``true" i-band fluxes using maximum 
likelihood (using the likelihood defined in section~\ref{sec:gen_model}). We used this estimated $i$-band flux and the Hubble F606W-band flux to define the ``true" color. We again compared with the catalog returned by DAOPHOT, PCAT, and wake-sleep. We see that DAOPHOT did not capture the full spectrum of colors; of all three methods, PCAT seemed to best capture the color spectrum. All three methods however, were able to capture the arm thing (TODO there is a name for this ... main sequence turnoff?) that branches off at low magnitudes. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/cmd.png}
    \caption{Color magnitude diagrams on M2. }
    \label{fig:cmd_m2}
\end{figure}


\subsection{Estimation of model parameters}
\label{sec:results_model_params}
In this section, we examine our estimated model parameters. As shown in the previous section, wake-sleep improved the catalog over sleep-only. In table~\ref{tab:chi-square-stats1}, we
print the negative log-likelihood $- \log \Prob_\phi(X | N_{H}, \ell_{H}, f_{H})$ for various model parameters $\phi$, where $N_{H}, \ell_{H}, f_{H}$ are the ground truth
values from the Hubble catalog (though the $i$-band flux needs to be estimated, as above). 

We compared the log-likelihood under the default SDSS estimates with the estimates found using wake-sleep. We see that the wake-sleep estimates improved the log-likelihood. We also compared against the ``Hubble estimate" of the background and PSF, obtained by minimizing 
$- \log \Prob_\phi(X | N_{H}, \ell_{H}, f_{H})$ for $\phi$ directly. 

This table suggests that the largest source of model misfit is the background. A significant decrease in negative log-likelihood occurred by switching from the SDSS background to the Hubble-estimated background. 
But even using the Hubble-estimated background, switching from the SDSS PSF to our wake-sleep PSF still improved the log-likelihood. 

\input{tables/chi_sq_stats.txt}

As evidenced in figure~\ref{fig:psf_profiles}, our wake-sleep PSF did not change the SDSS PSF significantly. The greatest change was in the r-band PSF, where the SDSS PSF was most different from the Hubble-estimated PSF. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/psf_profiles.png}
    \caption{Estimated versus true PSF profiles on M2. The Hubble PSF was
    obtained by optimizing the likelihood conditioned on locations and fluxes
    from the Hubble catalog. }
    \label{fig:psf_profiles}
\end{figure}


% \multicolumn{1}{p{5cm}}{\raggedleft Neg. loglik \\ (with Hubble back.)}
% \caption{
% Chi-squared statistics for SDSS, wake-sleep, and Hubble estimated model parameters.
% The chi-squared statistic is defined as
% $\sum_{bij}\frac{([\text{obs.image}]_{bij} - [\text{recon.image}]_{bij})^2}{[\text{recon.image}]_{bij}}$.
% In the middle column, ``model parameters" refer to both background and PSF.
% In the right column, we fix the background to the Hubble estimate, and examine
% chi-squared statistics as the PSF varies.}

\subsection{Sparse field test}
As discussed in section~\ref{sec:runtime}, our approach has the potential 
to scale up to large astronomical surveys. We chose the image of M2 to 
test our method because ``ground truth" could be obtained from Hubble images 
to validate our results. Most regions of the sky are much less 
crowded than M2.

We applied our method to SDSS run 94, camcol 1, and field  12, 
a region of the sky with star density more typical of the SDSS survey. 
After 10 minutes of training, we produced a catalog on the full $1489\times 2048$ image in $\approx2$ seconds. 

We chose this SDSS image because it is contained in Stripe 82, a region of the sky repeated imaged by SDSS. Averaging images from different runs boosts the signal to noise ratio and this {\itshape co-added} image can be analyzed to obtain a ground truth. 

However, this region of the sky also contains galaxies, which are 
not well-modelled by a PSF. A future paper will extend our generative model to include galaxies. For our setup, we use our approximate posterior 
trained using sleep-only (otherwise the model will try to use the PSF to explain both stars and galaxies in the wake-phase). 

Since this region of the sky is more sparse, we tile the image into $50\times 50$ tiles; $N_{max}$ on the tiles is three. 

In figure~\ref{fig:sparse_field}, we plot a $500\times 500$ subimage with 
our MAP catalog overlayed with the ground truth. We also report the 
TPR, comparing the stars in the co-added catalog with our estimated stars. (We have extraneous defections that are due to galaxies and thus 
do not report the PPV). Of the stars in the co-added catalog with brightness at least $22.5$mag, we achieved a TPR of 90\%. 


\begin{figure}
    \centering
    \begin{subfigure}[!t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/sparse_field_detections.png}
        \label{fig:sparse_field_detect}
    \end{subfigure}
    \begin{subfigure}[!t]{0.54\textwidth}
        \includegraphics[width=\textwidth]{figures/sparse_field_tpr.png}
        \label{fig:sparse_field_tpr}
    \end{subfigure}
    \caption{(Left) a $500\times 500$ sparse field, with true stars in 
    blue, and true galaxies in green. Estimated stars are shown in red x's. 
    (Right) the true positive rate as a function of true magnitude. }
    \label{fig:sparse_field}
\end{figure}
