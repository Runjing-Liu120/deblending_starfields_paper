\subsection{The Generative model}
\label{sec:gen_model}
To generate a $H \times W$ pixel image, we first draw stars from a marked Poisson process
on the plane. Explicitly, we first draw the number of stars $N$
\begin{align}
	N &\sim \text{Poisson}(\mu HW)
\end{align}
and independently, for every $N = 1, 2, ... $, we draw locations and fluxes
\begin{align}
  \ell_{N, i} &\sim \text{Uniform}([0, H] \times [0, W]) \quad \forall i = 1, ..., N \\
  f_{N, i}^{0} & \sim \text{Pareto}(f_{min}, \alpha) \quad \forall i = 1, ..., N
\end{align}
Note that we have a triangular array of latent variables; there is a set of
locations and fluxes for each $N$.

For images with $B$ bands, we also draw $B - 1$ colors,
\begin{align}
  c_{N, i}^{b}  & \sim \mathcal{N}(\mu_c, \sigma^2_c) \quad \forall b = 1, ..., B - 1; i = 1, ..., N.
\end{align}
Given flux in the first band, $f^0$, the flux in the $b$th band $f^b$ is given by
$f^0 \times 10^{c^b / 2.5}$.

To generate an image, having sampled $N\sim \text{Poisson}(\mu HW)$,
we index into the $N$th row of the triangular array of latent variables.
We use this set of locations, fluxes, and colors to produce an image.
The expected number of photoelectrons to arrive at pixel grid
coordinate $(h,w)$ in band $b$ is given by
\begin{align}
  \lambda^b_{hw} = I^{b}_{hw} + \sum_{i = 1}^N f_{N, i}^b \mathcal{P}^b\big(h - \ell_{N, i}[1], w - \ell_{N,i}[2]\big)
  \label{eq:expected_intensity}
\end{align}
where $I^{b}_{hw}$ is the background intensity, which we allow to vary by pixel and band,
and $\mathcal{P}^b$ is the point spread function (PSF) for band $b$. The PSF
is a function $\mathcal{P}^b : [0, H] \times [0, W] \mapsto \Reals^+$,
describing how a stellar point source appears
on our image. We shall see in the sequel that these model
parameters $I^{b}_{hw}$ and $\mathcal{P}$ will be estimated jointly with
the latent variables.

Making an appeal to the law of rare events, we model the
observed number of photoelectrons at pixel grid coordinate $(h,w)$ and band $b$ as Poisson
with mean $\lambda^b_{hw}$. Since $\lambda^b_{hw}$ is large,
we take the Gaussian approximation to the Poisson.
In sum, the observed pixel intensities are drawn
\begin{align}
  X_{hw}^b | f, \ell, N \overset{ind}{\sim} \mathcal{N}(\lambda^b_{hw}, \lambda^b_{hw}).
\end{align}

\subsection{Variational inference}
We have thus far described the likelihood for the data given latent variables,
$\Prob(X | N, \ell, f)$ and specified the prior $\Prob(N, \ell, f)$. Our goal is
the posterior distribution $\Prob(N, \ell, f | X)$. For our proposed model,
the exact posterior distribution is intractable.

Markov chain Monte Carlo (MCMC) is a common approach for approximating
posterior distributions by constructing a stochastic process on the parameter space
whose stationary distribution is the true posterior. However,
its computational cost is prohibitively large for
large scale astronomical surveys. We propose to construct an approximate posterior
using variational inference, resulting in a procedure that is several orders of
magnitude faster than MCMC.

Variational inference posits a family of distributions $\mathcal{Q}$ and seeks
the distribution $q^*\in \mathcal{Q}$ that is closest to the true posterior
in $\KL$ divergence.


% \subsection{Estimation of the catalogue}
% In previous work \cite{Brewer_2013, Portillo_2017, Feder_2019}, the posterior on
% the latent variables $(N, \ell, f, c)$ was approximated using MCMC. In \cite{Portillo_2017, Feder_2019},
% a method was proposed to further reduce the posterior samples to a single point estimate
% which they call a {\itshape condensed catalogue}.
%
% While MCMC allows for the careful quantification of uncertainties, its computational cost
% is prohibitively large for large scale astronomical surveys. One possible alternative
% is to characterize the posterior using the maximum a posteriori estimate. Using the
% generative model from section~\ref{sec:gen_model}, the joint loglikelihood is
% \begin{align}
%   \log \mathcal{L}(N, \ell, f,& c) \stackrel{c}{=} \overbrace{\sum_{b = 1}^B \sum_{w = 1}^W \sum_{h = 1}^H
%         \Big\{-\frac{1}{2}\log{\lambda_{hw}^b} - \frac{(x_{hw}^b - \lambda_{hw}^b)^2}{2\lambda_{hw}^b}\Big\}}^
%         \text{Gaussian likelihood} + ...\notag\\
%         & ... + \underbrace{N\log(\mu HW) + \log N!}_\text{Poisson prior on $N$} -
%         \underbrace{\sum_{i = 1}^N (\alpha + 1)\log f_{N, i}^b}_\text{Pareto prior on fluxes} +
%         \underbrace{\sum_{b = 1}^B \sum_{i = 1}^N \frac{(c_{N, i}^b - \mu_c)^2}{2\sigma_c^2}}_\text{Gaussian prior on color}
% \end{align}
%
% To maximize this joint-loglikelihood, we must optimize over a discrete random variable $N$,
% so the usual gradient-based optimization methods do not apply. Indeed, it would require
% optimizing the locations, fluxes, and colors for each $N$ independently, and comparing
% the resulting log-likelihoods across $N$.
%
% We propose a method to approximate the maximum a posteriori estimate. Let
% $f:x \mapsto (\hat N, \hat \ell)$ be the function that maps data $x$ to the MAP estimates of
% the $N$ and $\ell$. In our procedure, we first train a neural network $q$ to approximate $f$.
% Thus, obtaining estimates for $N$ and $\ell$ at inference time is a computationally efficient
% forward pass through a neural network. With estimates in hand for $N$ and $\ell$,
% optimizing for $f$ can be done quickly with a few (quasi)-Newton steps; recalling
% our model for the photoelctron counts, equation~\ref{eq:expected_intensity}, we see that
% are are simply regressing the observed image onto a linear combination of PSFs,
% and the coefficients of this linear combination are the desired fluxes.
