In mean-field variational inference, the E-step in equation~\eqref{eq:e_step} is often optimized by coordinate descent in the parameters of the variational distribution. 
The coordinate descent updates can be derived in closed form in the special case of exponential family models that are conditionally conjugate~\cite{Blei_2017_vi_review}. 

In our setting of amortized variational inference, stochastic 
optimization procedures have been employed with modern 
auto-differentiation tools to avoid the need for deriving 
analytic updates. Examples include black-box variational inference (BBVI)~\cite{ranganath2013black} 
and automatic-differentiation variational inference (ADVI)~\cite{kucukelbir2016automatic}. The latter 
is closely related to the reparameterization trick that \cite{kingma2013autoencoding, rezende2014stochastic} proposed to train deep generative models using the KL objective. 
These approaches all sample latent variables from $q_\eta$ and produce an unbiased estimate 
for the gradient of the ELBO (equation~\eqref{eq:elbo}); the optimization is done with stochastic gradient descent. 

However, the reparameterization trick does not apply when a latent variable is discrete, in our case, the number of stars $N$. Alternatively, the REINFORCE estimator~\cite{Williams1992reinforce}, which BBVI adapts, produces an unbiased stochastic gradient for both continuous and discrete latent variables. However, it often suffers from high variance in practice and the resulting stochastic optimization is slow. 

Thus, we propose an alternative objective function inspired by 
the wake-sleep algorithm originally proposed by~\cite{Hinton1995wake_sleep}. 
In the {\itshape sleep phase}, we optimize  
\begin{align}
    \argmin_{\eta} \Expect_{\tilde X \sim \Prob(\tilde X)}\Big[\KL(\Prob(N, \ell, f | \tilde X) \| q_\eta(N, \ell, f | \tilde X)\Big]. 
    \label{eq:sleep_obj}
\end{align}

We comment on two key differences between the sleep objective and the classical variational inference objective (equation~\ref{eq:kl_objective}). First note the difference in ordering of arguments to the $\KL$. 
While optimizing $\KL(q\|p)$ in equation~\eqref{eq:kl_objective} does not depend on the intractable 
data likelihood $\Prob(X)$, the $\KL$ with arguments reversed does. 
As will be detailed in section~\ref{sec:sleep_details}, the outer expectation over the data makes this optimization objective tractable with various desirable properties. 

The expectation over the data also gives different meaning to the sleep objective. The classical VI objective seeks to minimize the $\KL$ between $P(N, \ell, f | X)$ and $q(N, \ell, f | X)$ {\itshape for fixed, observed data $X$}, 
in this case the $H\times W$ image. In contrast, the sleep objective seeks to minimize the $\KL$ {\itshape averaged over all possible data. }

Moreover, ``possible data" is defined under our generative model from section~\ref{sec:gen_model}. In order for $q(N, \ell, f | X)$ to be a good approximation to
the true posterior when $X$ is observed data, then it is imperative that our generative model $\Prob(X)$ well-approximates the true underlying data-generating mechanism. 

In our setting, we tune two model parameters in the data generating process: the background intensity $I$, and the PSF $\mathcal{P}$. The background at pixel $(i, j)$ is taken to be an affine function in $i$ and $j$; we PSF is modeled as a weighted average between a Gaussian ``core" and power-law ``wings" as in~\cite{Xin2018psf}. See appendix REF for details. We concatenate all model parameters into a real-valued vector denoted $\phi$. 

For fixed $q_\eta$, these model parameters are estimated in the {\itshape wake-phase} by solving 
\begin{align}
    \phi_{t} = \argmax_{\phi} \; \Expect_{q_{\eta}(N, \ell, f | X)}\Big[\log \Prob_\phi(X, N, \ell, f)\Big]. 
    \label{eq:wake_elbo}
\end{align}
Note that here, the expectation is taken over the latent variables under $q_\eta$, while the data $X$ is fixed at its observed value. 

Equation~\eqref{eq:wake_elbo} is equivalent to maximizing the ELBO in equation~\eqref{eq:elbo} with respect to model parameters $\phi$. 
Since maximizing the ELBO is equivalent to 
minimizing the $\KL$, we can also express the wake-phase objective as 
\begin{align}
    \phi_{t} = \argmin_{\phi} \; \mathrm{KL}\Big[\,q_\eta(N, \ell, f | X)\, \| \,\Prob_\phi(N, \ell, f | X )\,\Big]. 
\end{align}

In summary, given an initial $\eta_0$, $\phi_0$, and data $X$,
for $t = 1, ..., T$, the wake-sleep algorithm alternates between two objectives: 
\begin{align}
    \text{{\bf Sleep: }} & 
    \eta_{t} = \argmin_\eta \; \Expect_{\Prob_{\phi_{t - 1}}(\tilde X)}\Big[\KL\big ( \Prob_{\phi_{t - 1}}(N, \ell, f | \tilde X)\| q_{\eta}(N, \ell, f |\tilde X)\big)\Big]
    \label{eq:sleep_kl}\\
    \text{{\bf Wake: }} & \phi_{t} =  \argmin_{\phi} \; \mathrm{KL}\Big[\,q_{\eta_t}(N, \ell, f | X)\, \| \,\Prob_\phi(N, \ell, f | X )\,\Big]. 
    \label{eq:wake_kl}
\end{align}
In the sleep phase, we are simulating data from $\Prob_\phi$ (i.e. ``dreaming") in 
order to construct an approximate posterior. In the wake phase, we reveal the observed data $X$ and optimize model parameters $\phi$ so that our data-generating process aligns as closely as possible with the true data. 

\subsubsection{Decomposing the sleep objective}
\label{sec:sleep_details}
In this section, we decompose the sleep-phase objective in equation~\ref{eq:sleep_obj} to examine its properties. First, we show 
that the objective does not depend on the intractable term $\Prob(X)$. 
We write
\begin{align}
 \argmin_{\eta} \; & \mathbb{E}_{\tilde X \sim P(\tilde X)}\Big[ \mathrm{KL}(P(N, \ell, f | \tilde X) \| q_\eta(N, \ell, f | \tilde X)\Big] \\
  &=\argmin_{\eta} \; \mathbb E_{P(\tilde X)}\Big[\mathbb E_{P(N, \ell, f | \tilde X)}\Big(\log P(N, \ell, f | \tilde X) - \log q_\eta(N, \ell, f | \tilde X) \Big)\Big]\\
&=\argmin_{\eta} \; \mathbb E_{P(\tilde X)}\Big[\mathbb E_{P(N, \ell, f | \tilde X)}\Big( - \log q_\eta(N, \ell, f | \tilde X) \Big)\Big]\\
&=\argmin_{\eta} \; \mathbb E_{P(\tilde X, N, \ell, f)}\Big[- \log q_\eta(N, \ell, f | \tilde X) \Big]\label{eq:sleep_loss_simple}
\end{align}
Crucially, the integrating distribution is $P$, which does not depend on the optimization parameter $\eta$.
In the classical variational inference objective, equation~\eqref{eq:kl_objective},
the integrating distribution is $q_\eta$, resulting in the need for reparameterization or other adjustments to compute stochastic gradients. 
Here, unbiased stochastic gradients can be obtained simply as 
\begin{align}
    g = -\nabla_\eta \log q_\eta(N, \ell, f | \tilde X) \quad \text{ for } (N, \ell, f, \tilde X)\sim P
\end{align}

In other words, we simulate {\itshape complete} data $(N, \ell, f, \tilde X)$ from our generative model and optimize $q_{\eta}(N, \ell, f | \tilde X)$ so that it maps images $\tilde X$ to a probability distribution with mass on $(N, \ell, f)$.

Recall that $-\log q_\eta(N, \ell, f | X)$ factorizes over image patches, and 
for a given image patch, the latent variables $N$, $\ell$, $f$ also factorize (for the moment dropping dependence on the patch index $k$). Having sampled $N,\ell,f,\tilde X$ from $P$, the loss in equation~\eqref{eq:sleep_loss_simple} becomes
\begin{align}
    - \log q_\eta(N, \ell, f | \tilde X) = 
        - \log q_\eta(N | \tilde X) 
        - \log q_\eta(\ell | \tilde X) 
        - \log q_\eta(f | \tilde X) 
        \label{eq:sleep_loss_decomp}
\end{align}
We examine each term separately. The number of stars $N$ is categorical with parameter $\omega$. The loss function for the number of stars becomes
\begin{align}
    - \log q_\eta(N | \tilde X) = -\sum_{n = 0}^{N_{max}} 1\{N = n\} \log \omega_n, 
\end{align}
the usual cross-entropy loss for a multi-class classification problem. 

For the last two terms, recall that locations are logit-normal and fluxes are log-normal. 
For a given index $(i, N)$ let $y_{i,N}$ generically denote either the 
logit-location or log-flux for that star, 
and let $\mu$ and $\sigma^2$ generically denote its variational parameters. 
Then the losses in the last two terms of equation~\ref{eq:sleep_loss_decomp}
of the form 
\begin{align}
    -\log q_\eta(y_{i,N} | \tilde X) = 
        \frac{1}{2\sigma^2}(y_{i,N} - \mu)^2 + \frac{1}{2}\log(2\pi\sigma^2)
\end{align}
We also give an interpretation: $y_{i,N}$ is the true, simulated parameter value (either logit-location or log-flux); $\mu$ is the predicted value of the neural network. $\sigma^2$ is also outputted by the neural network, representing uncertainty -- the second term encourages small uncertainties, but this is 
balanced by the scaling of the error $(y_{i,N} - \mu)^2$ in the first term. 

In summary, the sleep-phase results in a supervised learning problem on complete data simulated from our generative model. 

% \begin{itemize}
%     \item Number of stars $N$ is {\color{red}{\bf categorical}} with parameter $\omega$. 
%     \item Locations $\ell_n$ are {\color{blue}{\bf logitnormal}} with parameters $\mu_{\ell, n}$ and $\nu_{\ell, n}$
%     \item Fluxes $f_n$ are {\color{orange}{\bf lognormal}} with parameters $\mu_{f, n}$ and $\sigma^2_{f, n}$. 
% \end{itemize}
