In mean-field variational inference, the E-step in \eqref{eq:e_step} is often optimized by coordinate descent in the parameters of the variational distribution. 
The coordinate descent updates can be derived in closed form in the special case of exponential family models that are conditionally conjugate~\cite{Blei_2017_vi_review}. 

In our setting of amortized variational inference, stochastic 
optimization procedures have been employed with modern 
auto-differentiation tools to avoid the need for deriving 
analytic updates. Examples include black-box variational inference (BBVI)~\cite{ranganath2013black} 
and automatic-differentiation variational inference (ADVI)~\cite{kucukelbir2016automatic}. The latter 
is closely related to the reparameterization trick that \cite{kingma2013autoencoding, rezende2014stochastic} proposed to train deep generative models using the KL objective. 
These approaches all sample latent variables from $q_\eta$ and produce an unbiased estimate 
for the gradient of the KL; the optimization is done with stochastic gradient descent. 

However, the reparameterization trick does not apply when a latent variable is discrete, in our case, the number of stars $N$. Alternatively, the REINFORCE estimator~\cite{Williams1992reinforce}, which BBVI adapts, produces an unbiased stochastic gradient for both continuous and discrete latent variables. However, these gradients often suffer from high variance in practice, and so the resulting stochastic optimization is slow. 

Stochastic gradients are difficult to compute in the E-step because the integrating distribution depends on the optimization parameter, $\eta$. 
The wake-sleep algorithm, originally proposed by~\cite{Hinton1995wake_sleep}, replaces the 
E-step objective with 
\begin{align}
    \argmin_{\eta} \Expect_{x \sim p(x)}\Big[\KL(p_\phi(\theta | x) \| q_\eta(\theta | x)\Big]
    \label{eq:sleep_obj},
\end{align}
called the {\itshape sleep phase}. We will see in Section~\ref{sec:sleep_details} that a simple gradient estimator can be derived for the sleep phase objective, without 
reparameterization or REINFORCE. 

We comment on two key differences between the sleep objective~\eqref{eq:sleep_obj} and the E-step~\eqref{eq:e_step_kl}. First note the difference in ordering of arguments to the $\KL$. 
While optimizing $\KL(q\|p)$ in equation~\eqref{eq:e_step} does not depend on the intractable 
data likelihood $p_\phi(x)$, the $\KL$ with arguments reversed does. 
As will be detailed in Section~\ref{sec:sleep_details}, the outer expectation over the data makes this optimization objective tractable without dependence on $\log p(x)$. 

The expectation over the data also gives different meaning to the sleep objective. The E-step objective seeks $\eta$ to minimize the $\KL$ between and $q_\eta(\theta | x)$ $p_\phi(\theta | x)$ {\itshape for fixed, observed data $x$},
in this case the $H\times W$ image. In contrast, the sleep objective seeks to minimize the $\KL$ {\itshape averaged over all possible data. } In other words, the target is no longer an approximate posterior for observed data, but rather an approximate posterior that is ``good on average" over all possible data.

Moreover, ``possible data" is defined under our generative model from Section~\ref{sec:gen_model}. It is imperative that our generative model $p_\phi(x)$ well-approximates the true underlying data-generating mechanism. 

Therefore, in addition to the sleep-phase, we also incorporate the M-step~\eqref{eq:m_step} to estimate model parameters. In the context of the wake-sleep algorithm, the M-step is known as the ``wake-phase."

In summary, the {\itshape wake-sleep} algorithm alternates between the two objectives 
\begin{align}
    \text{{\bf Sleep-phase: }} & 
    \eta_{t} = \argmin_{\eta} \Expect_{x \sim p(x)}\Big[\KL(p_{\phi_{t-1}}(\theta | x) \| q_\eta(\theta | x)\Big]
    \label{eq:sleep_phase_summary}
    \\
    \text{{\bf Wake-phase: }} & \phi_{t} = \argmax_{\phi}\; \Expect_{q_{\eta_t}(z | x)}\Big[\log p_{\phi}(x, z) - \log q_{\eta_t}(z | x)\Big]
    \label{eq:wake_phase_summary}
\end{align} 
for iterations $t = 1, ..., T$. 

Stochastic gradients are easy to compute in the wake phase (M-step). Because the integrating distribution does not depend on the optimization parameter $\phi$ in the wake phase, unbiased stochastic gradients are computed simply as 
\begin{align}
    \nabla_\phi \log p_\phi(x, \theta) \quad \text{ for } \theta\sim q_\eta. 
    \label{eq:mstep_grad}
\end{align}
While the E-step is not conducive to simple gradient estimators, we will see in Section~\ref{sec:sleep_details} that our sleep-phase objective
results in a similarly straightforward gradient estimator as in~\eqref{eq:mstep_grad}. However,
we note that a consequence of replacing the E-step with the sleep phase is that the wake-sleep algorithm no longer targets the singular objective like the ELBO~\eqref{eq:em_obj}. 

% Equation~\eqref{eq:wake_elbo} is equivalent to maximizing the ELBO in equation~\eqref{eq:elbo} with respect to model parameters $\phi$. 
% Since maximizing the ELBO is equivalent to 
% minimizing the $\KL$, we can also express the wake-phase objective as 
% \begin{align}
%     \phi_{t} = \argmin_{\phi} \; \mathrm{KL}\Big[\,q_\eta(\theta | x)\, \| \,p_\phi(\theta | x )\,\Big]. 
% \end{align}

% In summary, given an initial $\eta_0$, $\phi_0$, and data $x$,
% for $t = 1, ..., T$, the wake-sleep algorithm alternates between two objectives: 
% \begin{align}
%     \text{{\bf Sleep: }} & 
%     \eta_{t} = \argmin_\eta \; \Expect_{p_{\phi_{t - 1}}(\tilde x)}\Big[\KL\big ( p_{\phi_{t - 1}}(\theta | \tilde x)\| q_{\eta}(\theta |\tilde x)\big)\Big]
%     \label{eq:sleep_kl}\\
%     \text{{\bf Wake: }} & \phi_{t} =  \argmin_{\phi} \; \mathrm{KL}\Big[\,q_{\eta_t}(\theta | x)\, \| \,p_\phi(\theta | x )\,\Big]. 
%     \label{eq:wake_kl}
% \end{align}
% order to construct an approximate posterior. In the wake phase, we reveal the observed data $x$ and optimize model parameters $\phi$ so that our data-generating process aligns as closely as possible with the true data. 

\subsubsection{Decomposing the sleep objective}
\label{sec:sleep_details}
In this section, we decompose the sleep-phase objective in~\eqref{eq:sleep_obj} to examine its properties. 
We take $\phi$ as fixed in this section, and drop the explicit dependence of $p$ on $\phi$.

First, we show that the objective does not depend on the intractable term $p(x)$. 
We write
\begin{align}
 \argmin_{\eta} \; & \mathbb{E}_{x \sim p(x)}\Big[ \mathrm{KL}(p(\theta | x) \| q_\eta(\theta | x)\Big] \\
  &=\argmin_{\eta} \; \mathbb E_{p(x)}\Big[\mathbb E_{p(\theta | x)}\Big(\log p(\theta | x) - \log q_\eta(\theta | x) \Big)\Big]\\
&=\argmin_{\eta} \; \mathbb E_{p(x)}\Big[\mathbb E_{p(\theta | x)}\Big( - \log q_\eta(\theta | x) \Big)\Big]\\
&=\argmin_{\eta} \; \mathbb E_{p(x, \theta)}\Big[- \log q_\eta(\theta | x) \Big]\label{eq:sleep_loss_simple}
\end{align}
Crucially, the integrating distribution is $p$, which does not depend on the optimization parameter $\eta$.
In the E-step of variational EM, the integrating distribution is $q_\eta$, resulting in the need for reparameterization or other adjustments to compute stochastic gradients. 
Here, unbiased stochastic gradients can be obtained simply as 
\begin{align}
    g = -\nabla_\eta \log q_\eta(\theta | x) \quad \text{ for } (\theta, x)\sim p
\end{align}

In other words, we simulate {\itshape complete} data $(\theta, x)$ from our generative model and evaluate the loss $-\log q_\eta(\theta | x)$. 
Here, ``complete data" refers to the image along with its catalog. 
The loss encourages $q_{\eta}(\theta | x)$ to map images $x$ to a probability distribution with large mass on the 
catalog $\theta$.

We examine the loss $-\log q_\eta(\theta | x)$ more closely. Recall that $q_\eta$ factorizes over image tiles. Having sampled the catalog $\theta$ 
and the $H\times W$ image $x$ from $p$, we convert $\theta$ to its tile parameterization $(N^{(s,t)}, \ell^{(s,t)}, f^{(s,t)})_{s=1,t=1}^{(S,T)}$ as detailed in Section~\ref{sec:factorization}.

For a given image tile, the latent variables $N^{(s,t)}$, $\ell^{(s,t)}$, $f^{(s,t)}$ also factorize in $q$, so 
\begin{align}
- \log q_\eta(N^{(s,t)}, \ell^{(s,t)}, f^{(s,t)} | x) =         - \log q_\eta(N^{(s,t)} | x) 
        - \log q_\eta(\ell^{(s,t)} | x) 
        - \log q_\eta(f^{(s,t)} | x). 
        \label{eq:sleep_loss_decomp}
\end{align}
We examine each term separately. On tile $(s,t)$, the number of stars $N^{(s,t)}$ is categorical with parameter $\omega^{(s,t)}$. The loss function for the number of stars becomes
\begin{align}
    - \log q_\eta(N^{(s,t)} | x) = -\sum_{n = 0}^{N_{max}} 1\{N^{(s,t)} = n\} \log \omega^{(s,t)}_n, 
\end{align}
the usual cross-entropy loss for a multi-class classification problem. 

For the last two terms in~\eqref{eq:sleep_loss_decomp}, recall that location coordinates are logit-normal and fluxes are log-normal. For a given index $(N,i)$ let $y^{(s,t)}_{N,i}$ generically denote either the 
logit-location or log-flux for that star, 
and let $\mu^{(s,t)}_{N,i}$ and $\sigma^{k}_{N,i}$ generically denote its variational parameters,
the mean and standard deviation of a Gaussian. Then the losses in the last two terms of~\eqref{eq:sleep_loss_decomp} are of the form 
\begin{align}
    -\log q_\eta(y^{(s,t)}_{N,i} | x) = 
        \frac{1}{2(\sigma^{k}_{N,i})^2}(y^{(s,t)}_{N,i} - \mu^{(s,t)}_{N,i})^2
         + \log\sigma^{k}_{N,i}
         + \frac{1}{2}\log(2\pi).
\end{align}
By our discussion in Section~\ref{sec:factorization}, 
only the $N = N^{(s,t)}$-th row of the triangular 
array of latent variables needs to be evaluated. 

We also give an interpretation: $y_{N,i}$ is the true, simulated latent variable (either logit-location or log-flux); $\mu^{(s,t)}_{N,i}$ is the predicted value for that latent variable from the neural network. $\sigma^{k}_{N,i}$ is also outputted by the neural network, representing uncertainty -- the second term encourages small uncertainties, but this is 
balanced by the scaling of the error $(y^{(s,t)}_{N,i} - \mu^{(s,t)}_{N,i})^2$ in the first term. 

In summary, the sleep phase results in a supervised learning problem on complete data simulated from our generative model. 

\subsubsection{Reverse versus forward KL}
\label{sec:kl_q_p}
TODO: something about forward KL overestimates uncertainties, see Figure~\ref{fig:kl_q_p_schematic}. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width = 0.5\textwidth]{figures/kl_q_p_schematic.png}
    \caption{A toy example where the true distribution is a bivariate Gaussian on 
    $\theta = (\theta_1, \theta_2)$ with positively correlated components. 
    $q$ is a mean-field variational approximation. Left, the optimal $q$ found 
    optimizing $\KL(q\|p)$; right, the optimal $q$ found optimizing $\KL(p\|q)$. }
    \jeff{Are the contour lines for $p$ and $q$ the same (e.g., the contour that contains 50\% of the mass)? If so, is it true that these contours touch at exactly two point (rather than 0 or 4)? We should be correct on this detail if we're going to the trouble of showing a plot, even if we're just trying to get intuition.}
    \label{fig:kl_q_p_schematic}
\end{figure}

% \begin{itemize}
%     \item Number of stars $N$ is {\color{red}{\bf categorical}} with parameter $\omega$. 
%     \item Locations $\ell_n$ are {\color{blue}{\bf logitnormal}} with parameters $\mu_{\ell, n}$ and $\nu_{\ell, n}$
%     \item Fluxes $f_n$ are {\color{orange}{\bf lognormal}} with parameters $\mu_{f, n}$ and $\sigma^2_{f, n}$. 
% \end{itemize}
