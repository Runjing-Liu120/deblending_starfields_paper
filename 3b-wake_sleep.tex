In mean-field variational inference, the ELBO~\eqref{eq:elbo} is often optimized by coordinate ascent in the parameters of the variational distribution. 
The coordinate ascent updates can be derived in closed form in the special case of exponential family models that are conditionally conjugate~\cite{Blei_2017_vi_review}. 

In our setting of amortized variational inference, stochastic 
optimization procedures have been employed with modern 
auto-differentiation tools to avoid the need for deriving 
analytic updates. Examples include black-box variational inference (BBVI)~\cite{ranganath2013black} 
and automatic-differentiation variational inference (ADVI)~\cite{kucukelbir2016automatic}. The latter 
is closely related to the reparameterization trick \cite{kingma2013autoencoding, rezende2014stochastic} proposed to train deep generative models using the KL objective. 
These approaches all sample latent variables from $q_\eta$ and produce an unbiased estimate 
for the gradient of the KL; the optimization is done with stochastic gradient descent. 

However, the reparameterization trick does not apply when a latent variable is discrete -- in our case, the number of stars $N$. The REINFORCE estimator~\cite{Williams1992reinforce}, which BBVI adapts, produces an unbiased stochastic gradient for both continuous and discrete latent variables. However, these gradients often suffer from high variance in practice, and so the resulting stochastic optimization is slow. See Appendix~\ref{sec:reparam_details} for details concerning both the reparameterization and REINFORCE estimators. 

Stochastic gradients of the ELBO are difficult to compute because the integrating distribution depends on the optimization parameter, $\eta$. 
{\itshape The wake-sleep algorithm}, originally proposed by~\cite{Hinton1995wake_sleep}, replaces the 
ELBO objective with 
\begin{align}
    \mathcal{L}_{sleep}(\eta) := 
    - \Expect_{x \sim p(x)}\Big[\KL(p(z | x) \| q_\eta(z | x)\Big]
    \label{eq:sleep_obj},
\end{align}
known as the {\itshape sleep objective}. 
Section~\ref{sec:sleep_details} details a simple gradient estimator for the $\mathcal{L}_{sleep}$, without reparameterization or REINFORCE. 

There are two key differences between the sleep objective~\eqref{eq:sleep_obj} and the ELBO~\eqref{eq:elbo}. 
Recall maximizing the ELBO is equivalent to minimizing
$\KL(q\|p)$, and 
therefore, the minimization of $\KL(q\|p)$ does not depend on the intractable data likelihood $p(x)$. 
However, the minimization of $\KL$ with arguments reversed does. 
As will be detailed in Section~\ref{sec:sleep_details}, the outer expectation over the data in~\eqref{eq:sleep_obj} makes this optimization objective tractable without dependence on $p(x)$. 

The expectation over the data also gives different meaning to the sleep objective. The ELBO objective seeks $\eta$ to minimize the $\KL$ between $q_\eta(z | x)$ and $p(z | x)$ {\itshape for fixed, observed data $x$},
in this case the $H\times W$ image. In contrast, the sleep objective seeks to minimize the $\KL$ {\itshape averaged over all possible data. } In other words, the target is no longer an approximate posterior for observed data, but rather an approximate posterior that is ``good on average" over all possible data.

Moreover, ``possible data" is defined under the generative model from Section~\ref{sec:gen_model}. Therefore, it is imperative that the generative model $p(x)$ well-approximates the true underlying data-generating mechanism. 

Thus, in addition to the sleep phase, the wake-sleep algorithm also incorporates a ``wake-phase" to estimate model parameters.
In our application, these model parameters include the PSF parameters $\pi$ and background parameters $\beta$. Let 
$\phi:=(\pi, \beta)$ be the concatenation of all model parameters, and denote the dependence of the generative model on $\phi$ using subscripts, $p_\phi$. 

To estimate model parameters, one would ideally optimize the marginal log-likelihood $\log p_\phi(x)$.
However, since $\log p_\phi(x)$ is intractable, the wake-phase optimizes for $\phi$ using the ELBO objective~\eqref{eq:elbo}
as a surrogate for the intractable log-likelihood. 
The ELBO is a lower bound
$\log p_\phi(x)$; it is equal to $\log p_\phi(x)$ when $q_\eta(z | x)$ 
= $p_\phi(z | x)$. 

Putting the pieces together, the {\itshape wake-sleep} algorithm alternates between the two objectives: 
\begin{align}
    \text{{\bf Sleep phase: }} & 
    \eta_{t} = \argmax_{\eta} -\Expect_{x \sim p_\phi(x)}\Big[\KL(p_{\phi_{t-1}}(z | x) \| q_\eta(z | x)\Big]
    \label{eq:sleep_phase_summary}; 
    \\
    \text{{\bf Wake phase: }} & \phi_{t} = \argmax_{\phi}\; \Expect_{q_{\eta_t}(z | x)}\Big[\log p_{\phi}(x, z) - \log q_{\eta_t}(z | x)\Big],
    \label{eq:wake_phase_summary}
\end{align} 
for iterations $t = 1, ..., T$. 

Stochastic gradients of the expectation in the wake-phase are are simple to compute. Because the integrating distribution does not depend on the optimization parameter $\phi$ in the wake phase, unbiased stochastic gradients are computed simply as 
\begin{align}
    \nabla_\phi \log p_\phi(x, z) \quad \text{ for } z\sim q_\eta. 
    \label{eq:mstep_grad}
\end{align}
Section~\ref{sec:sleep_details} shows that a similarly simple gradient estimator exists for the sleep objective.


% While the E-step~\eqref{eq:e_step} is not conducive to simple gradient estimators, we will see in Section~\ref{sec:sleep_details} that the sleep phase objective
% results in a similarly straightforward gradient estimator as in~\eqref{eq:mstep_grad}. 
% In Section~\ref{sec:estep_sleep_compare}, 
% we compare optimizing the E-step with optimizing the sleep phase. In our application of localizing stars,
% the ELBO objective in the E-step suffers from poor local optima where the variational distribution is far from the true posterior in KL divergence; the sleep phase objective appears to better avoid these local optima. 
